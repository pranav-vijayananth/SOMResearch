{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_som_labels.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1H8irdk772gijgZPwAROIq79y0oilYUfD",
      "authorship_tag": "ABX9TyPFOoHIjQS8MGspYDNP81mV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranav-vijayananth/SOMResearch/blob/main/tensorflow_som_labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WywA5I8AxVXy"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/cgorman/tensorflow-som/master/tf_som.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8hi9B_vxbJa"
      },
      "source": [
        "!pip3 install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sU-xaA8xbz1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tf_som import SelfOrganizingMap\n",
        "from scipy.spatial import distance_matrix\n",
        "import subprocess\n",
        "from collections import Counter\n",
        "import time \n",
        "import logging\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQT7qW3Uxg8o"
      },
      "source": [
        "#FUNCTION FOR GETTING HEAT MAP OF CLUSTERS AND THE BMU LOCATIONS\n",
        "def get_umatrix(input_vects, weights, m, n):\n",
        "    umatrix = np.zeros((m * n, 1))\n",
        "    neuron_locs = list()\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            neuron_locs.append(np.array([i, j]))\n",
        "\n",
        "    neuron_distmat = distance_matrix(neuron_locs, neuron_locs)\n",
        "\n",
        "    for i in range(m * n):\n",
        "        neighbor_idxs = neuron_distmat[i] <= 1  \n",
        "        neighbor_weights = weights[neighbor_idxs]\n",
        "        umatrix[i] = distance_matrix(np.expand_dims(weights[i], 0), neighbor_weights).mean()\n",
        "\n",
        "    bmu_indices = []\n",
        "    for vect in input_vects:\n",
        "        min_index = min([i for i in range(len(list(weights)))],\n",
        "                        key=lambda x: np.linalg.norm(vect-\n",
        "                                                     list(weights)[x]))\n",
        "        bmu_indices.append(neuron_locs[min_index])\n",
        "        \n",
        "    return umatrix, bmu_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFJxTbJTxpZ4"
      },
      "source": [
        "data = []\n",
        "groundtruth = []\n",
        "dataread = datafile.readline()\n",
        "dataread = datafile.readline()\n",
        "\n",
        "while dataread != \"\": \n",
        "  a = dataread.split(\",\")\n",
        "  l2 = []\n",
        "  for j in range(0, len(a), 1):\n",
        "    if j == len(a)-1:\n",
        "      groundtruth.append(a[j].strip())\n",
        "    else:\n",
        "      try: \n",
        "        l2.append(float(a[j]))\n",
        "      except:\n",
        "        l2.append(0)\n",
        "  data.append(l2)\n",
        "  dataread = datafile.readline()\n",
        "\n",
        "rows = len(data)\n",
        "cols = len(data[0])\n",
        "num_inputs = rows * cols\n",
        "\n",
        "datafile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwZw4pCOxpzk"
      },
      "source": [
        "print(f\"This is the data: {data}\")\n",
        "print(f\"Number of cols: {cols}\")\n",
        "print(f\"These are the number of rows {rows}\")\n",
        "print(f\"This is the grountruth: {groundtruth}\")\n",
        "print(f\"Number of input vectors: {num_inputs}\")\n",
        "\n",
        "#CLUSTERS\n",
        "uniqueValues = Counter(groundtruth).keys()\n",
        "clusters = len(uniqueValues)\n",
        "print(f\"The clusters in dataset: {clusters}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7jSAo5txq2R"
      },
      "source": [
        "#CREATING THE TENSORFLOW GRAPH AND LOGS FOR THE MODEL\n",
        "\n",
        "# for i in range(1, 30+1):\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "  session = tf.Session(config=tf.ConfigProto(\n",
        "            allow_soft_placement=True,\n",
        "            log_device_placement=False\n",
        "  ))\n",
        "\n",
        "  #BATCH SIZE \n",
        "  batch_size = 32\n",
        "\n",
        "  #CONVERT TO NUMPY ARRAY FOR TF DATA PIPELINE\n",
        "  data = np.array(data, dtype=\"float32\")\n",
        "\n",
        "  #MAKING THE TENSORFLOW DATRASET PIPELINE \n",
        "  input_data = tf.data.Dataset.from_tensor_slices(data)\n",
        "  input_data = input_data.repeat()\n",
        "  input_data = input_data.batch(batch_size)\n",
        "  iterator = input_data.make_one_shot_iterator()\n",
        "  next_element = iterator.get_next()\n",
        "\n",
        "  #BUILDING THE SOM OBJECT\n",
        "  som = SelfOrganizingMap(m=clusters, n=clusters, dim=cols, max_epochs=20, session=session, graph=graph, input_tensor=next_element, batch_size=batch_size, initial_learning_rate=0.1, model_name='Self-Organizing-Map', softmax_activity=True)\n",
        "\n",
        "  #MAKING + RUNNING SESSION\n",
        "  init_op = tf.global_variables_initializer()\n",
        "  session.run([init_op])\n",
        "\n",
        "  #TRAINING\n",
        "  som.train(num_inputs)\n",
        "\n",
        "  #WEIGHTS\n",
        "  som_weights = som.output_weights\n",
        "\n",
        "  #SOM UMATRIX + BMU LOCATION/PREDICTION\n",
        "  umatrix, bmu_loc = get_umatrix(data, som_weights, clusters, clusters)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}