{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_som_weights.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1afW4gtNuBXukr6R41lFzqPfON4Jdk0eA",
      "authorship_tag": "ABX9TyOCT963SLDwSuISizmEAKl8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranav-vijayananth/SOMResearch/blob/main/tensorflow_som_weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AKOascAqsW7"
      },
      "source": [
        "#HARDWARE SPECS INFORMATION\n",
        "\n",
        "print(\"### CPU INFO ###\")\n",
        "print(\"--------------------------------------------------\")\n",
        "!cat /proc/cpuinfo\n",
        "print(\"### MEMORY INFO ###\")\n",
        "print(\"--------------------------------------------------\")\n",
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQLsTeVpfxww"
      },
      "source": [
        "#CHECKING IF CUDA IS RUNNING\n",
        "\n",
        "!nvidia-smi\n",
        "!python3 --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntF9t9wurGd-"
      },
      "source": [
        "#INSTALLING TENSORFLOW-SOM MODULE\n",
        "\n",
        "!wget https://raw.githubusercontent.com/cgorman/tensorflow-som/master/tf_som.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOSVXElhyJ8A"
      },
      "source": [
        "#NEED PYTHON 3.7 FOR THIS VERSION OF TF API\n",
        "\n",
        "!pip3 install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1fl_oDF4i9R"
      },
      "source": [
        "#IMPORTS\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tf_som import SelfOrganizingMap\n",
        "import subprocess\n",
        "from collections import Counter\n",
        "import logging\n",
        "import glob\n",
        "import time\n",
        "\n",
        "print(tf.__version__)\n",
        "print(np.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP9cZr-gFdjm"
      },
      "source": [
        "#FUNCTION FOR GETTING FILE IN PATH\n",
        "\n",
        "def get_file_in_path(path):\n",
        "  return path.split(\"/\")[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mi3WQTk2CPe"
      },
      "source": [
        "def get_name_in_path(filename):\n",
        "  return filename.split(\".\")[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jZzUFlcO0kw"
      },
      "source": [
        "#GETTING ALL FILE PATHS IN THE FOLDER\n",
        "\n",
        "master_files = glob.glob(\"/content/drive/MyDrive/Datasets/commonfiles/*.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t47w4VNJ5BLK"
      },
      "source": [
        "#GETTING FILE PATHS AND NAMES\n",
        "\n",
        "file_path = master_files[0]\n",
        "filename = get_file_in_path(file_path)\n",
        "name = get_name_in_path(filename) \n",
        "\n",
        "print(f\"This is the file path: {file_path}\")\n",
        "print(f\"Name of the file: {filename}\")\n",
        "print(f\"Name without file format: {name}\")\n",
        "\n",
        "datafile = open(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C24jCfpb1cYa"
      },
      "source": [
        "#MAKING FOLDER FOR WEIGHTS\n",
        "\n",
        "bash_command = f\"mkdir /content/drive/MyDrive/Datasets/Research/CommonFilesWeights/{name}\"\n",
        "subprocess.run(bash_command, shell=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnZ3th5V5C9a"
      },
      "source": [
        "#CLEANING DATA\n",
        "\n",
        "data = []\n",
        "groundtruth = []\n",
        "dataread = datafile.readline()\n",
        "dataread = datafile.readline()\n",
        "\n",
        "while dataread != \"\": \n",
        "  a = dataread.split(\",\")\n",
        "  l2 = []\n",
        "  for j in range(0, len(a), 1):\n",
        "    if j == len(a)-1:\n",
        "      groundtruth.append(a[j].strip())\n",
        "    else:\n",
        "      try: \n",
        "        l2.append(float(a[j]))\n",
        "      except:\n",
        "        l2.append(0)\n",
        "  data.append(l2)\n",
        "  dataread = datafile.readline()\n",
        "\n",
        "rows = len(data)\n",
        "cols = len(data[0])\n",
        "num_inputs = (rows * cols)\n",
        "\n",
        "datafile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLwC-BoM5FFG"
      },
      "source": [
        "#GOOGLE COLAB DOES NOT SUPPORT TO EXTEND VIEWFRAME FOR HIGH AMOUNTS OF DATA\n",
        "\n",
        "# print(f\"This is the data: {data}\")\n",
        "print(f\"Number of cols: {cols}\")\n",
        "print(f\"These are the number of rows {rows}\")\n",
        "print(f\"This is the grountruth: {groundtruth}\")\n",
        "print(f\"Number of input vectors: {num_inputs}\")\n",
        "\n",
        "#CLUSTERS\n",
        "uniqueValues = Counter(groundtruth).keys()\n",
        "clusters = len(uniqueValues)\n",
        "print(f\"The clusters in dataset: {clusters}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UScrhB0d5I8v"
      },
      "source": [
        "#CREATING THE TENSORFLOW GRAPH AND LOGS FOR THE MODEL\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "  session = tf.Session(config=tf.ConfigProto(\n",
        "            allow_soft_placement=True,\n",
        "            log_device_placement=False\n",
        "  ))\n",
        "\n",
        "  #NEURONS FOR FEATURE MAP \n",
        "  neurons = int(5*math.sqrt(rows))\n",
        "  m = int(math.sqrt(neurons)+1)\n",
        "  n = int(math.sqrt(neurons)+1)\n",
        "  neurons = (m*n)\n",
        "\n",
        "  #BATCH SIZE \n",
        "  batch_size = 32\n",
        "\n",
        "  #CONVERT TO NUMPY ARRAY FOR TF DATA PIPELINE\n",
        "  data = np.array(data, dtype=\"float32\")\n",
        "\n",
        "  #MAKING THE TENSORFLOW DATRASET PIPELINE \n",
        "  input_data = tf.data.Dataset.from_tensor_slices(data)\n",
        "  input_data = input_data.repeat()\n",
        "  input_data = input_data.batch(batch_size)\n",
        "  iterator = input_data.make_one_shot_iterator()\n",
        "  next_element = iterator.get_next()\n",
        "\n",
        "# ONE TXT FILE = ONE WEIGHT FOR EVERY ITERATION\n",
        "\n",
        "  for i in range(1, 30+1):\n",
        "    #MAKING FILE TO STORE WEIGHTS FOR EVERY ITERATION \n",
        "    weights_file = f\"{name}-{i}.txt\"\n",
        "    bash_command = f\"touch /content/drive/MyDrive/Datasets/Research/CommonFilesWeights/{name}/{weights_file}\"\n",
        "    subprocess.run(bash_command, shell=True)\n",
        "\n",
        "    #BUILDING THE SOM OBJECT\n",
        "    som = SelfOrganizingMap(m=clusters, n=1, dim=cols, max_epochs=11, session=session, graph=graph, input_tensor=next_element, batch_size=batch_size, initial_learning_rate=0.1, model_name='Self-Organizing-Map', softmax_activity=True)\n",
        "\n",
        "    #MAKING + RUNNING SESSION\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    session.run([init_op])\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    #TRAINING\n",
        "    som.train(num_inputs)\n",
        "\n",
        "    stop = time.time()\n",
        "\n",
        "    #TIME \n",
        "    print(f\"Training time: {stop - start}s\")\n",
        "\n",
        "    #WEIGHTS\n",
        "    som_weights = som.output_weights\n",
        "    \n",
        "    #STORING WEIGHTS\n",
        "    wf = open(f\"content/drive/MyDrive/Datasets/Research/CommonFilesWeights/{name}/{weights_file}\", \"a\")\n",
        "    for w in som_weights:\n",
        "      str_w = [str(ele) for ele in w]\n",
        "      temp_w = \",\".join(str_w)\n",
        "      wf.write(temp_w)\n",
        "      wf.write(\"\\n\")\n",
        "      \n",
        "    wf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_FGaFpg5JTE"
      },
      "source": [
        "# OUTPUT WEIGHTS #\n",
        "\"\"\" \n",
        "The output_weights format satisfies the given rows and columns for the neuron inputs. \n",
        "Use the shape property to return the tuple of the given weight form \n",
        "Create a file I/O function to iterate the weights into txt files\n",
        "Iteration should be over 30 and epochs should be changed to 11 for faster computation\n",
        "\"\"\"\n",
        "\n",
        "weights = som.output_weights\n",
        "print(weights.shape == (neurons, cols))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J6cn0gJpZxx"
      },
      "source": [
        "# SSH CONNECTION\n",
        "\n",
        "\"\"\"\n",
        "For SSH connection you need to use the UCID and the hostname which is the given server. Using this you can acess the other information given\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}