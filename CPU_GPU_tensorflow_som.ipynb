{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPU/GPU-tensorflow-som.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1uEKVUJDsGSJ0RsLX6lzh5ZZnEQDIe8Nh",
      "authorship_tag": "ABX9TyM1d13sBcDN4EFjH1vGVBxY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranav-vijayananth/SOMResearch/blob/main/CPU_GPU_tensorflow_som.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av_k_Of2xZNQ"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIfjOYvOrWVR"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/cgorman/tensorflow-som/master/tf_som.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofuW4g1eHG3Q"
      },
      "source": [
        "!pip3 install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBHwbXhfqSrF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tf_som import SelfOrganizingMap\n",
        "from scipy.spatial import distance_matrix\n",
        "from collections import Counter\n",
        "import logging\n",
        "import subprocess\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvGtcDvdxljz"
      },
      "source": [
        "print(tf.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XNUW2H7rwxZ"
      },
      "source": [
        "# GETTING LABELS FROM PREDICTION\n",
        "\n",
        "def get_labels(prediction):\n",
        "    labels = []\n",
        "    \n",
        "    for arr in prediction:\n",
        "        c=0\n",
        "        for label in arr:\n",
        "            c+=1\n",
        "            if(c==2):\n",
        "                labels.append(label)\n",
        "    \n",
        "    labels = \",\".join(list(map(lambda item: str(item), labels)))\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT05khFfsLhl"
      },
      "source": [
        "#HEAT MAP OF CLUSTERS AND BMU LOCATIONS\n",
        "\n",
        "def get_umatrix(input_vects, weights, m, n):\n",
        "    umatrix = np.zeros((m * n, 1))\n",
        "    neuron_locs = list()\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            neuron_locs.append(np.array([i, j]))\n",
        "\n",
        "    neuron_distmat = distance_matrix(neuron_locs, neuron_locs)\n",
        "\n",
        "    for i in range(m * n):\n",
        "        neighbor_idxs = neuron_distmat[i] <= 1\n",
        "        # print(\"index: \",neighbor_idxs)\n",
        "        # print(\"shape: \",weights.shape)\n",
        "        neighbor_weights = weights[neighbor_idxs]\n",
        "        umatrix[i] = distance_matrix(np.expand_dims(weights[i], 0), neighbor_weights).mean()\n",
        "\n",
        "    bmu_indices = []\n",
        "    for vect in input_vects:\n",
        "        min_index = min([i for i in range(len(list(weights)))],\n",
        "                        key=lambda x: np.linalg.norm(vect-\n",
        "                                                     list(weights)[x]))\n",
        "        bmu_indices.append(neuron_locs[min_index])\n",
        "        \n",
        "    return umatrix, bmu_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsOWcnIgsR1C"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Datasets/Research/SelectedDatasets/veteran.csv\"\n",
        "\n",
        "files = path.split(\"/\")[-1]\n",
        "name = files.split(\".\")[0]\n",
        "\n",
        "print(name)\n",
        "\n",
        "filename = open(path)\n",
        "\n",
        "data = []\n",
        "groundtruth = []\n",
        "dataread = filename.readline()\n",
        "dataread = filename.readline()\n",
        "\n",
        "while dataread != \"\": \n",
        "    a = dataread.split(\",\")\n",
        "    l2 = []\n",
        "    for j in range(0, len(a), 1):\n",
        "        if j == len(a)-1:\n",
        "            groundtruth.append(a[j].strip())\n",
        "        else:\n",
        "            try: \n",
        "                l2.append(float(a[j]))\n",
        "            except:\n",
        "                l2.append(0)\n",
        "    data.append(l2)\n",
        "    dataread = filename.readline()\n",
        "\n",
        "rows = len(data)\n",
        "cols = len(data[0])\n",
        "num_inputs = rows * cols\n",
        "\n",
        "print(f\"Rows: {rows}\")\n",
        "print(f\"Columns: {cols}\")\n",
        "print(f\"Neurons: {num_inputs}\")\n",
        "\n",
        "filename.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9rKQpNTvisJ"
      },
      "source": [
        "#CLUSTERS\n",
        "\n",
        "uniqueValues = Counter(groundtruth).keys()\n",
        "clusters = len(uniqueValues)\n",
        "print(f\"The clusters in dataset: {clusters}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRmmGIA9wixq"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "for i in tqdm(range(1, 100+1)):\n",
        "  #CREATING THE TENSORFLOW GRAPH AND LOGS FOR THE MODEL\n",
        "        \n",
        "  logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "  graph = tf.Graph()\n",
        "\n",
        "  with graph.as_default():\n",
        "      session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(\n",
        "                  allow_soft_placement=True,\n",
        "                  log_device_placement=False\n",
        "      ))\n",
        "\n",
        "      #BATCH SIZE \n",
        "      batch_size = 32\n",
        "      \n",
        "      #CONVERT TO NUMPY ARRAY FOR TF DATA PIPELINE\n",
        "      data = np.array(data, dtype=\"float32\")\n",
        "      \n",
        "      #MAKING THE TENSORFLOW DATRASET PIPELINE \n",
        "      input_data = tf.compat.v1.data.Dataset.from_tensor_slices(data)\n",
        "      input_data = input_data.repeat()\n",
        "      input_data = input_data.batch(batch_size)\n",
        "      iterator = input_data.make_one_shot_iterator()\n",
        "      next_element = iterator.get_next()\n",
        "      \n",
        "      #BUILDING THE SOM OBJECT\n",
        "      som = SelfOrganizingMap(m=1, n=clusters, dim=cols, max_epochs=20, session=session, graph=graph, input_tensor=next_element, batch_size=batch_size, initial_learning_rate=0.1,gpus=1, model_name='Self-Organizing-Map', softmax_activity=True)\n",
        "      \n",
        "      #MAKING + RUNNING SESSION\n",
        "      init_op = tf.compat.v1.global_variables_initializer()\n",
        "      session.run([init_op])\n",
        "      \n",
        "      #TRAINING\n",
        "      som.train(num_inputs)\n",
        "      \n",
        "      #SOM UMATRIX + BMU LOCATION/PREDICTION\n",
        "      umatrix, bmu_loc = get_umatrix(data, som.output_weights, 1, clusters)\n",
        "      labels = get_labels(bmu_loc)\n",
        "\n",
        "      bash_command = f\"touch /content/drive/MyDrive/Datasets/Research/GPU+CPULabels/CPU/{name}/{name}-{i}.txt\"\n",
        "      subprocess.run(bash_command, shell=True, check=True)\n",
        "\n",
        "      fo = open(f\"/content/drive/MyDrive/Datasets/Research/GPU+CPULabels/CPU/{name}/{name}-{i}.txt\", \"a\")\n",
        "      fo.write(labels)\n",
        "      fo.write(\"\\n\")\n",
        "      \n",
        "      fo.close()\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"{files} / CPU / {abs(start-stop)} \")\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}